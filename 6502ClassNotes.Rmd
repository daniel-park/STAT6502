---
title: "STAT 6502 Class Notes"
author: "Daniel Park"
date: '`r Sys.Date()`'
output:
  html_document: default
  pdf_document: default
header-includes:
- \usepackage{bbm}
- \usepackage{framed}
---

**January 04, 2016**

###Chapter 10: Data Summary  
We now tackle the problem of estimating the behavior of a random variable where we have no (or very little) information about the population that the measurements came from.

Recall the population is uniquely defined by the cumulative distribution.

The cumulative distribution of $X$ evaluated at $x$ is $P(X \leq x)$.

In statistics, we assume that the sample well represents the population.

Fuctions of a random variable in a sample should be similar to that of the population (e.g. the sample mean to the population mean).  

**Empirical Cumulative Distribution Function**  
This is the estimator of the cdf, abbreviated as ecdf, and is defined as
$$F_n(x) = \frac{1}{n}(\text{number of} \ x_i\text{'s that are equal to or less than} \ x)$$
and has the properties

- let the ordered observations be $x_{(1)}, x_{(2)}, ... , x_{(n)}$.  Then for $x < x_{(1)}, F_n(x) = \frac{1}{n}0= 0$.  
- for $x_{(k)} \leq x < x_{(k+1)}, F_n(x)=\frac{k}{n}$.  
- if there are $r$ observations with the value $x$, then $F_n(x)$ has a jump of $\frac{r}{n}$ at $x$.  

We can show the ecdf is a consistent estimator of the cdf.

As $n$ approaches infinity, for fixed $\epsilon > 0$, Chebyshev's inequality gives
$$
\begin{aligned}
\lim_{n \to \infty} P\big(\big|F_n(x)-F(x) \big|<\epsilon\big) &= \lim_{n \to \infty} P\bigg(\bigg|\frac{Y}{n}-p\bigg| < \epsilon\bigg) \\
                                              &\geq \lim_{n \to \infty} \bigg(1 - \frac{p(1-p)}{n\epsilon^2}\bigg) \\
                                              &= 1
\end{aligned}
$$

Therefore,
$$\lim_{n \to \infty} P\big(\big|F_n(x) - F(x)\big| < \epsilon \big) = 1$$

$\Big($ The derivation resulted in the inequality $\lim_{n \to \infty} P\big(\big|F_n(x) - F(x)\big| < \epsilon \big) \geq 1$.  However, since probabilities cannot exceed 1, the inequality becomes  $\lim_{n \to \infty} P\big(\big|F_n(x) - F(x)\big| < \epsilon \big) = 1$.$\Big)$

Since $F_n(x) = \hat{p}$ is the proportion of times that $X \leq x$, we may rely on the Weak Law of Large Numbers (WLLN), which states that $F_n(x)$ will converge in probability to $F(x)=p$, or $F_n(x) \xright{p} F(x)$.

Reminder: $\xrightarrow{p}$ means that it converges in probability.

In fact, the consistency of $F_n$ holds uniformly over the real line
$$\sup_{- \infty < x < \infty} \big|F_n(x) - F(x) \big| \xrightarrow{p} 0$$

Demonstration in R:
```{r}
# randomly selecting numbers, intentionally including two 16s:
x <- c(16, 16, 14, 3.5, -1, 2.2, 35, 47, 50, 21)
plot(ecdf(x))
# Repeat, but this time randomly generate the numbers.
# We generate 30 observations, each with 20 trials.
# The expected value of each observation is 6; that is, with a probability
#   of 0.3, we expect 6 successes from 20 trials.
y <- rbinom(n=30, size=20, prob=.3)
y
plot(ecdf(y), xlim=c(0,20))

# actual distribution:
points(x=0:20, y=pbinom(q=0:20, size=20, prob=.3), col='red') 
```

For mathematical purposes, it is convenient to express $F_n$ as 
$$F_n(x) = \frac{1}{n} \sum_{i=1}^n \textbf{I}_{(-\infty, x]}(X_i)$$
where $\textbf{I}$ is the usual indicator function.
$$
\textbf{I}_{(-\infty, x]}(X_i)=\begin{cases}
                              1, \text{if} \ X_i \leq x \\
                              0, \text{if} \ X_i > x
                               \end{cases}
$$

Writing it in this way, we see that $\textbf{I}_{(-\infty, x]}(X_i)$ is a Bernoulli random variable with
$$P(\textbf{I}_{(-\infty, x]}(X_i) = 1) = F(x)$$
$$\text{and}$$
$$P(\textbf{I}_{(-\infty, x]}(X_i) = 0) = 1 - F(x)$$

```{r eval=FALSE, echo=FALSE}
# Note use \mathbbm{1} to make it correctly formatted as a pdf
```


As an estimator, the ecdf is not only consistent, it is unbiased, and its variance gets smaller the further $x$ is from the median.


**January 06, 2016**  

**Mean and Variance**  
Problem: Show that the ecdf is unbiased for the cdf.
$$
\begin{aligned}
E(F_n) &= E\bigg[\frac{1}{n} \sum_{i=1}^n \textbf{I}_{(-\infty,x]}(X_i)\bigg] \\
       &= \frac{1}{n}nE[\textbf{I}_{(-\infty,x]}(X_i)] \\
       &= P(X_i \leq x) \\
       &= F(x)
\end{aligned}
$$
For the variance, note that since $\textbf{I}_{(-\infty,x]}(X_i)$ can only take values 0 or 1, $E[(\textbf{I}_{(-\infty,x]}(X_i))^2] = E[\textbf{I}_{(-\infty,x]}(X_i)]$ which is equal to $P(X_i \leq x)$.  

Therefore,  
$$
\begin{aligned}
\text{Var}(F_n(x)) &= E[F^2_n(x)]-[E(F_n(x))]^2 \\
                   &= E\bigg[\frac{1}{n^2} \sum_{i=1}^n \textbf{I}_{(-\infty,x]}(X_i) \sum_{j=1}^n \textbf{I}_{(-\infty, x]}(X_j)\bigg]-F^2(x) \\
                   &= \frac{1}{n^2} \sum_{i=1}^n \sum_{j=1}^n[P(X_i \leq x, X_j \leq x)-F^2(x)] \\
                   &= \frac{1}{n^2} \sum_{i=1}^n F(x)(1-F(x))+ \frac{1}{n^2} \sum_{i \neq j}[F_{X_iX_j}(x,x)-F^2(x)]
\end{aligned}
$$
If they are independent, then
$$F_{X_iX_j}(x,x)=F^2(x)$$
which means that
$$\sum_{i \neq j}[F_{X_iX_j}(x,x)-F^2(x)] = 0$$
So,
$$
\begin{aligned}
\text{Var}(F_n(x)) &= \frac{1}{n^2} \sum_{i=1}^n F(x)(1-F(x)) \\
                   &= \frac{1}{n} F(x)(1-F(x))
\end{aligned}
$$

It is clear that the variance hits its maximum when $F(x)=0.5$, otherwise known as the median.  

Also, $\text{Var}(F_n(x))$ gets smaller as you go away from the median.  

A drawback is that the ecdf is always a step-function, whereas the cdf of a continuous distribution is not, so we may lose out on important features.  

```{r}
curve(pnorm, -3, 3) # cdf of normal dist
x <- rnorm(300) # 300 observations
plot(ecdf(x))
curve(pnorm, -3,3,add=TRUE,col='red') # true cdf
```

The plotted ecdf are steps.  Note that typically the true cdf is unknown (which is why we're interested in the estimator ecdf).

As an estimator, the ecdf can be used to estimate many functional parameters of $F$.  

**Example**  
Suppose that
$$\theta(F)=\int_{-\infty}^{\infty} h(x)dF(x)$$
which is essentially $E[h(x)]$.  

Substituting $F_n$ for $F$, we get
$$
\begin{aligned}
\hat{\theta} &= \theta(F_n) \\
             &= \int^{\infty}_{-\infty} h(x)dF_n(x) \\
             &= \frac{1}{n} \sum_{i=1}^n h(x_i)
\end{aligned}
$$
which is simply the sample mean of $h(x_1), h(x_2),...,h(x_n)$.  

Here, $\theta=\theta(F)$ is the true value of the parameter, and $\hat{\theta}=\theta(F_n)$ is its estimator.

**Example**  
Suppose that $\theta(F)=\text{Var}(X)$ where $X \sim F$.
$$\theta(F)=\int^{\infty}_{-\infty} x^2 dF(x)-\bigg(\int_{-\infty}^{\infty}xdF(x)\bigg)^2$$
and we get
$$
\begin{aligned}
\hat{\theta} &= \theta(F_n) \\
             &= \frac{1}{n}\sum_{i=1}^n x_i^2-\bigg(\frac{1}{n}\sum_{i=1}^n x_i\bigg)^2 \\
             &= \frac{1}{n} \sum_{i=1}^n \big(x_i-\overline{X}\big)^2
\end{aligned}
$$

**Survival**  
The survival function is defined as
$$S(t)=P(T>t)=1-F(t)$$  
where $T$ is a random variable with cdf $F$.  

This is commonly found in biostatistics and reliability theory as $S(t)$ represents the probability that the lifetime will be longer than $t$.

From our prevous discurssion, the intuitive estimator of the survival function is
$$S_n(t)=1-F_n(t)$$
which is consistent and unbiased, the proportion of the data greater than $t$.  

```{r eval=FALSE, echo=FALSE}
library(ggplot2)
library(reshape2)
library(survival)
library(ggfortify)
raw1 <- "
76
93
97
107
108
113
114
119
136
137
138
139
152
154
154
160
164
164
166
168
178
179
181
181
183
185
194
198
212
213
216
220
225
225
244
253
256
259
265
268
268
270
283
289
291
311
315
326
326
361
373
373
376
397
398
406
452
466
592
598
"

raw2 <- "
72
72
78
83
85
99
99
110
113
113
114
114
118
119
123
124
131
133
135
137
140
142
144
145
154
156
157
162
162
164
165
167
171
176
177
181
182
187
192
196
211
214
216
216
218
228
238
242
248
256
257
262
264
267
267
270
286
303
309
324
326
334
335
358
409
473
550
"

raw3 <- "
10
33
44
56
59
72
74
77
92
93
96
100
100
102
105
107
107
108
108
108
109
112
113
115
116
120
121
122
122
124
130
134
136
139
144
146
153
159
160
163
163
168
171
172
176
183
195
196
197
202
213
215
216
222
230
231
240
245
251
253
254
254
278
293
327
342
347
361
402
432
458
555
"

raw4 <- "
43
45
53
56
56
57
58
66
67
73
74
79
80
80
81
81
81
82
83
83
84
88
89
91
91
92
92
97
99
99
100
100
101
102
102
102
103
104
107
108
109
113
114
118
121
123
126
128
137
138
139
144
145
147
156
162
174
178
179
184
191
198
211
214
243
249
329
380
403
511
522
598
"

raw5 <- "
12
15
22
24
24
32
32
33
34
38
38
43
44
48
52
53
54
54
55
56
57
58
58
59
60
60
60
60
61
62
63
65
65
67
68
70
70
72
73
75
76
76
81
83
84
85
87
91
95
96
98
99
109
110
121
127
129
131
143
146
146
175
175
211
233
258
258
263
297
341
341
376
"

rawcontrol <- "
18
36
50
52
86
87
89
91
102
105
114
114
115
118
119
120
149
160
165
166
167
167
173
178
189
209
212
216
273
278
279
292
341
355
367
380
382
421
421
432
446
455
463
474
506
515
546
559
576
590
603
607
608
621
634
634
637
638
641
650
663
665
688
725
735
"

guinea.pig <- data.frame(control=c(read.table(text=rawcontrol)$V1, rep(NA, 7)),
                         dose1=c(read.table(text=raw1)$V1, rep(NA, 12)),
                         dose2=c(read.table(text=raw2)$V1, rep(NA, 5)),
                         dose3=read.table(text=raw3)$V1,
                         dose4=read.table(text=raw4)$V1,
                         dose5=read.table(text=raw5)$V1
                         )
# Alternative: use scan to read in a column of text as a vector

guinea.pig <- melt(guinea.pig, variable.name="category", value.name="lifetimes")
guinea.surv <- survfit(Surv(lifetimes) ~ category, data = guinea.pig)
autoplot(object=guinea.surv, conf.int=FALSE)

ggplot(data=guinea.pig, aes(x=lifetimes,color=category)) + stat_ecdf(geom="step")
```


A survival function can be used to calculate the hazard function, which may be interpreted as the instantaneous death rate for individuals who have suvived up to a given time.

For small $\delta$, we calculate the probability that the individual will die in the time interval $(t,t+\delta)$
$$
\begin{aligned}
P(t\leq T \leq t + \delta | T>t) &= \frac{P(t \leq T \leq t+\delta)}{P(T>t)} \\
                                 &= \frac{F(t+\delta)-F(t)}{1-F(t)} \\
                                 &\approx \frac{\delta f(t)}{1-F(t)}
\end{aligned}
$$
The hazard rate function is defined as $h(t)=\frac{f(t)}{1-F(t)}$ and alternatively expressed as
$$h(t)=-\frac{d}{dt}\log(1-F(t)) = -\frac{d}{dt}\log(s(t))$$  

**Example**  
In R, plot the hazard rate function for the Weibull with $\alpha=1$ and $\beta=0.8,1.0,1.5$
```{r}
x <- seq(0,2,length=1000)
plot(x,dweibull(x,0.8,1)/(1-pweibull(x,0.8,1)), type='l',ylab='')
lines(x,dweibull(x,1,1)/(1-pweibull(x,1,1)),col='red')
lines(x,dweibull(x,1.5,1)/(1-pweibull(x,1.5,1)),col='green')
```

**Problem**  
Find the hazard rate function for exponentially distributed lifetimes.  What does this tell you?  
$$
\begin{aligned}
h(t) &= \frac{f(t)}{1-F(t)} \\
     &= \frac{\lambda e^{-\lambda t}}{1-(1-e^{-\lambda t})} \\
     &= \lambda
\end{aligned}
$$

**Density Estimation**  
The glaring issue with using the ecdf as an estimator of $F$ arises when $F$ is continuous (since the ecdf is always a step function).

In this situation, one way of deriving a smoothed estimate is using the kernel probability density estimate.  

The idea is to use $w(x)$, where this function is a nonnegative, symmetric weight function, centered at zero and integrates to 1 (pdf).  

**January 11, 2015**  
This $w$ uses the $X_i$ to come up with a smoothed estimate at $x$.  

However, we may not want to use values of $X_i$ that are too far from $X_j$ since those values won't tell us much about the distribution at $x$.  

Even for those values not too far away from $x$, we would want to weight values closest to $x$ more than those that are further away.  

Therefore, we need to decide on a bandwidth, $h$, and run this smoother in this bandwidth around $x$, on the known support $X$.

The choice of $h$ can have a profound effect on the quality of the density estimate.  

The choice of $h$ is problem specific.

We can write this weight function as
$$W_h(x) = \frac{1}{h}w\bigg(\frac{x}{h}\bigg)$$
giving our estimate of $f$,
$$
\begin{aligned}
f_h(x) &= \frac{1}{n}\sum_{i=1}^n W_h(x-X_i) \\
  &= \frac{1}{nh}\sum_{i=1}^n w\bigg(\frac{x-X_i}{h}\bigg)
\end{aligned}
$$
If $h$ is too small we will have undersmoothing, producing a rough $f$, not much different than the unsmoothed.

If $h$ is too large, then we will have oversmoothing, possibly losing out on some features of the true $f$.  

```{r}
n <- 100
x <- rchisq(n,8)
#h <- 1
h <- c(0.5, 1, 5)
t <- seq(0,25, length=n)
fhat1 <- matrix(0, nrow=n, ncol=length(h))

plot(t, dchisq(t,8), type='l')

for (j in 1:length(h)){
  for (i in 1:n){
		fhat1[i,j] = 1/(n*h[j])*sum(dnorm((t[i]-x)/h[j]))
		}
	lines(t, fhat1[,j], xlab='x', ylab='', col=j+1)
	}
	
```

Alternatively, we could use the `KernSmooth` library to accomplish the same thing.
```{r eval=FALSE}
# Alternatively

plot(density(x))

# or 

library(KernSmooth)

fhat3 <- bkde(x)
plot(fhat3, xlab='x', ylab='',type='l')
lines(t, dchisq(t,8), col='red')
```



**Nonparametric Bootstrap**  
Let $x_i,x_2,...,x_n$ be our sample.  

A bootstrap sample is a SRS of $n$ observations, taken with replacement, from the original sample, say $x^*_1,x^*_2,...,x^*_n$.  

Repeat this process a large number of times, $B$.  

For each sample, calculate the statistic of interest.  

The behavior of the statistic from bootstrap sample to bootstrap sample should be similar to the behavior of that statistic from sample to sample (sample distribution).  

For a given bootstrap sample, $x_i$ is represented $r_i$ times.  

Therefore, $\sum_{i=1}^n r_i=n$ and $r_i \sim \text{bin}\big(n, \frac{1}{n}\big)$.

The mean and variance of $r_i$ can be determined by the standard formulas for the binomial distribution.
$$E_B(r_i)= np = n\cdot\frac{1}{n}=1$$
$$\text{Var}_B(r_i) = \frac{p(1-p)}{n}= \frac{\frac{1}{n}\big(1-\frac{1}{n}\big)}{n}=\frac{\frac{1}{n}\big(\frac{n-1}{n}\big)}{n} = \frac{n-1}{n}$$
For $i\neq j, \ \text{Cov}(r_i,r_j)=\sigma_{ij}=-\frac{1}{n}$.  
This implies $E_B(\sigma_i,\sigma_j)=1-\frac{1}{n}$.  
Note: $r_i$'s are not independent b/c of the constraint $\sum_{i=1}^n r_i=n$.  If we knew, say, $r_1=5$, then we have some information about the distribution of the rest of the $r_i$'s.

Derivation of $\text{Cov}(r_i,r_j)$: We can determine this by setting $\text{Var}_B\big(\sum_{i=1}^n r_i\big)$ to 0 since $\text{Var}_B\big(\sum_{i=1}^n r_i\big)=\text{Var}_B(n)$, where $n$ is a constant and the variance of a constant is zero. 
$$
\begin{aligned}
0 &= \text{Var}_B\bigg(\sum_{i=1}^n r_i\bigg) \\
  &= \sum_{i=1}^n \text{Var}_B(r_i) + \sum_{i=1}^n \sum_{j \neq i} \text{Cov}_B(r_i,r_j) \\
  &= n\bigg(\frac{n-1}{n}\bigg)+n(n-1)\sigma_{ij} && \text{b/c} \ \text{Cov}_B(r_i,r_j)=\sigma_{ij} \\
  &= (n-1)+n\sigma_{ij}(n-1) \\
  &= 1+n\sigma_{ij} && \text{dividing equation by} \ (n-1) \\
-n\sigma_{ij} &= 1 \\
\sigma_{ij} &= -\frac{1}{n}
\end{aligned}
$$

**Example**  
In the following code, we generate 40 trials, each with 10 observations and success rate of 0.3.  From that pool of data, we sample from it to generate a binomial trial.  From each trial we calculate the standard deviation.  We do this 10,000 times and plot a histogram of our simulations.

```{r fig.width=5, fig.height=4}
x <- rbinom(n=40, size=10, prob=0.3)
sdstar <- numeric(length=10000)
for (i in 1:10000) {
  sdstar[i] <- sd(sample(x=x, size=40, replace=TRUE))
}
hist(sdstar)
```

The distribution appears normal.

**January 13, 2016**  
**Mean**  
The sample mean of a bootstrap sample is 
$$\overline{X}^*=\frac{1}{n}\sum_{i=1}^nr_ix_i$$
where $r_i$ is the number of observed values at $x_i$, and $x_i$ is treated as fixed.  

So, because $E_B(r_i)=1 \ \Big(\sum_{i=1}^n r_i=n$ and $r_i \sim \text{bin}\big(n, \frac{1}{n}\big)\Big)$, 
$$
\begin{aligned}
E_B\Big(\overline{X}^*\Big) &= E\bigg(\frac{1}{n}\sum_{i=1}^n r_ix_i\bigg) \\
  &= \frac{1}{n}\sum_{i=1}^n E_B(r_i)x_i \\
  &= \frac{1}{n}\sum_{i=1}^n x_i\cdot 1 \\
  &= \overline{x}
\end{aligned}
$$
and, because $\text{Cov}(x_i,x_j)=\frac{1}{n}$,
$$
\begin{aligned}
\text{Var}_B\Big(\overline{X}^*\Big) &=\frac{1}{n^2}\sum_{i=1}^nx^2_i\text{Var}_B(r_i)+\frac{1}{n^2}\sum_{i=j}^n\sum_{j\neq i} \text{Cov}(r_ix_i,r_jx_j) \\
  &= \frac{1}{n^2}\bigg[\sum_{i=1}^n x^2_i \bigg(\frac{n-1}{n}\bigg) + \sum_{i=1}^n\sum_{j \neq i}x_ix_j\bigg(-\frac{1}{n}\bigg)\bigg] \\
    &= \frac{n-1}{n^2}s^{*2}
\end{aligned}
$$
where 
$$s^{*2}=\frac{\frac{n-1}{n}\sum_{i=1}^n x_i^2 -\frac{1}{n} \sum_{i=1}^n\sum_{j \neq i}x_i x_j}{n-1}$$
Then the expected value is
$$
\begin{aligned}
E\Big[E_B\Big(\overline{X}^*\Big)\Big] &= E\Big(\overline{X}\Big) \\
  &=\mu
\end{aligned}
$$
which is unbiased.

The expeced value of $s^{*2}$
$$
\begin{aligned}
E\big(s^{*2}\big) &= E\Bigg[\frac{\frac{n-1}{n}\sum_{i=1}^n x_i^2 -\frac{1}{n} \sum_{i=1}^n\sum_{j \neq i}x_i x_j}{n-1}\Bigg] \\
  &= \frac{\frac{n-1}{n}\sum_{i=1}^n E(x_i^2) -\frac{1}{n} \sum_{i=1}^n\sum_{j \neq i}E(x_i x_j)}{n-1} \\
  &= \frac{\frac{n-1}{n}\sum_{i=1}^n (\sigma^2 + \mu^2) -\frac{1}{n} \sum_{i=1}^n\sum_{j \neq i}E(x_i)E(x_j)}{n-1} \\
  &= \sigma^2
\end{aligned}
$$
Note that $E(x_i x_j)=E(x_i)E(x_j)$ because of independence.

So $$E\Big[\text{Var}_B\Big(\overline{X}^*\Big)\Big]=\frac{n-1}{n^2}\sigma^2$$
Therefore the bias is $$\text{bias}_{\sigma^2_{\overline{X}^*}}=-\frac{1}{n^2}\sigma^2$$

**Example**  Compare CIs for the median and mean using bootstrap and classical methods assuming normality.  
```{r}
x <- rnorm(40, 12, 2)
medstar <- numeric(100000)
for (i in 1:10000) {medstar[i] <- median(sample(x, replace=TRUE))}
hist(medstar, freq=FALSE)
quantile(medstar, c(0.025, 0.975))
# we can do the following b/c of the histogram appears normal
median(x) + c(-1.96, 1.96)*sd(medstar) # semiparamtric
# In STAT 6304 we would do:
median(x) + c(qt(0.025, 39), qt(0.975, 39))*sd(x)/40
```
Use the bootstrap method if you cannot assume normality and the sample size is too small.

Using `boot` library
```{r}
library(boot)
fun.data <- function(w,i){median(x[i])}
y <- boot(x, fun.data, 1000)
boot.ci(y)
```

**Example** Suppose the population is exponential so that
$$f(x;\lambda)=\lambda e^{-\lambda x}$$
Create a 95% CI for the population median
$$
\begin{aligned}
  & F(t) = 0.5 \\
\Leftrightarrow  & 1-e^{-\lambda t} = 0.5 \\
\Leftrightarrow  & e^{-\lambda t} = 0.5 \\
\Leftrightarrow  & -\lambda t = \ln(0.5) \\
\Leftrightarrow  & t = -\frac{1}{\lambda}\ln(0.5)
\end{aligned}
$$

```{r eval=FALSE}
# Doesn't seem to work
q <- rexp(250, rate=17)
1/17*log(.5)
y <- boot(q, fun.data, 100000)
boot.ci(y)
```

**January 20, 2016**  
A different way of looking at something we've already seen.  

Suppose we have data generated as realizations of a random variable from a particular distribution parameterized by $\theta$.  

In addition, suppose we believe that there are only two possible values for $\theta$, say $\theta_1$ and $\theta_2$.  How do you decide which of these is the true $\theta$?  

From last quarter, we could calculate the likelihood of our data under both values of $\theta$, and select the one that has the larger likelihood.  

What we will do now is to take the ratio of those likelihood functions evaluated at the two $\theta$s and make our decision based on the value of this ratio.

**Example**  Suppose two siblings have identical phones.  The only observable difference is that the older one gets, on average, one text per hour whereas the younger gets, on average, 1.5 texts per hour.  In a rush to get to school, you see both phones next to each other and decide to take one of the phones.  After having the phone for 8 hours, how should you decide whose phone you took?  

Let $x$ denote the number of texts that arrive in 8 hours.  We have a Poisson distribution - we are given a rate and finite period of time.

Recall the Poisson pmf:
\[f(X=x)=\frac{\lambda^x e^{-\lambda}}{x!}\]

The parameter $\lambda$ represents the rate in a specified physical area or time frame.  Here, $\lambda=8$ for the older child (8 texts in an 8-hour period) and $\lambda=12$ for the younger child.

To get an idea about the distributions:
```{r fig.width=4, fig.height=3}
x <- 0:15 # considering probabilities for each of these counts
plot(x, dpois(x=x, lambda=8), type='h') # older child's phone
plot(x, dpois(x=x, lambda=12), type='h') #younger child's phone
```

Suppose we observed 9 text messages.

Poisson pmf: $f(X=x)=\frac{\lambda^x e^{-\lambda}}{x!}$  
Likelihood:
\[
\begin{aligned}
\frac{P_{younger}(x=9)}{P_{older}(x=9)} &= \frac{\frac{12^9e^{-12}}{9!}}{\frac{8^9e^{-8}}{9!}} \\
  &= \frac{`r dpois(x=9, lambda=12)`}{`r dpois(x=9, lambda=8)`} \\
  &= 0.704
\end{aligned}
\]
This is the likelihood ratio, so the younger sibling's phone is less likely to produce exactly 9 texts in 8 hours.  

On the other hand, if we had observed 11 texts, then 
$$\frac{P_{younger}(x=11)}{P_{older}(x=11)}=1.58$$
which means the younger sibling's phone is 1.58 times more likely to produce 11 texts in 8 hours.

When we have one hypothesis for each phone, $H_0$ and $H_1$, we have what is known as a **simple hypothesis**.  


To incorporate a Bayesian framework, we can specify a prior as to the probability of these hypotheses, $P(H_0)$ and $P(H_1)$, before observing the data.  

If we have no information, then
$$P(H_0)=P(H_1)=\frac{1}{2}$$
We can look at the ratio of posteriors where
$$P(H_0|x)=\frac{P(H_0,x)}{P(x)}=\frac{P(x|H_0)P(H_0)}{P(x)}$$
and likewise for $H_1$,
$$P(H_1|x)=\frac{P(x|H_1)P(H_1)}{P(x)}$$
we then have the ratio
$$\frac{P(H_0|x)}{P(H_1|x)}=\frac{P(x|H_0)}{P(x|H_1)}\frac{P(H_0)}{P(H_1)}$$

Suppose in our example that $H_0$ represents the belief that the phone belongs to the younger child.  Without prior probabilities:
\[
\begin{aligned}
\frac{P(H_0|x)}{P(H_1|x)} &=\frac{P(x|H_0)}{P(x|H_1)}\frac{P(H_0)}{P(H_1)} \\
  &= \frac{\frac{12^x e^{-12}}{x!}}{\frac{8^x e^{-8}}{x!}}\frac{\frac{1}{2}}{\frac{1}{2}}
\end{aligned}
\]
```{r echo=FALSE}
x <- 0:15
ratio <- round(dpois(x=x,lambda=12)/dpois(x=x,lambda=8), 2)
```

We can calculate the ratios at each value of $x$:

 x | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15
---|---|---|---|---|---|---|---|---|---|---|----|----|----|----|----|----
$\frac{P(H_0|x)}{P(H_1|x)}$|`r ratio[1]`|`r ratio[2]`|`r ratio[3]`|`r ratio[4]`|`r ratio[5]`|`r ratio[6]`|`r ratio[7]`|`r ratio[8]`|`r ratio[9]`|`r ratio[10]`|`r ratio[11]`|`r ratio[12]`|`r ratio[13]`|`r ratio[14]`|`r ratio[15]`|`r ratio[16]`  

```{r fig.width=4, fig.height=3}
x <- 0:15
plot(x=x, y=dpois(x=x,lambda=12)/dpois(x=x,lambda=8))
abline(h=1)
```

The horizontal line at $\frac{P(H_0|x)}{P(H_1|x)}=1$ represents the break even point. At 1, we have equal probabilities.  Values less than 1 favor the alternative hypothesis.  

Note that the graph is nonlinear and is a monotonically increasing function of $x$.  As $x$ increases, the likelihood increasingly favors the null hypothesis.  The points are not connected because $x$ is not continuous (you cannot have 5.5 texts).

Interpretation: If $x<10$, it is more likely that the older sibling's phone.

Now suppose that we believe that there was a one-third probability that we chose the younger child's phone ($H_0$).  Then if we were calculating Bayesian probabilities:
\[
\begin{aligned}
\frac{P(H_0|x)}{P(H_1|x)} &=\frac{P(x|H_0)}{P(x|H_1)}\frac{P(H_0)}{P(H_1)} \\
  &= \frac{\frac{12^x e^{-12}}{x!}}{\frac{8^x e^{-8}}{x!}}\frac{\frac{1}{3}}{\frac{2}{3}}
\end{aligned}
\]

```{r echo=FALSE}
ratio <- round(dpois(x=x, lambda=12)/dpois(x=x, lambda=8)*(1/3)/(2/3),2)
```


 x | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15
---|---|---|---|---|---|---|---|---|---|---|----|----|----|----|----|----
$\frac{P(H_0|x)}{P(H_1|x)}\frac{P(H_0)}{P(H_1)}$|`r ratio[1]`|`r ratio[2]`|`r ratio[3]`|`r ratio[4]`|`r ratio[5]`|`r ratio[6]`|`r ratio[7]`|`r ratio[8]`|`r ratio[9]`|`r ratio[10]`|`r ratio[11]`|`r ratio[12]`|`r ratio[13]`|`r ratio[14]`|`r ratio[15]`|`r ratio[16]`


```{r echo=3:7, fig.width=10, warning=FALSE}
default <- par()
par(mfrow=c(1,2))
x <- 0:15
plot(x=x, y=dpois(x=x, lambda=12)/dpois(x=x, lambda=8)*(1/3)/(2/3), 
     main="Likelihood Ratios with Bayesian Probabilities",
     xlab="Number of Texts", ylab="Likelihood Ratio", pch=3)
abline(h=1)
plot(x=x, y=dpois(x=x, lambda=12)/dpois(x=x, lambda=8)*(1/3)/(2/3),
     main="Likelihood Ratios of Bayesian and Frequentist Prob",
     xlab="Number of Texts", ylab="Likelihood Ratio", pch=3)
points(x=x, y=dpois(x=x, lambda=12)/dpois(x=x, lambda=8))
par(default)
```

The Bayesian likelihood has a more gradual rise because $\frac{P(H_0)}{P(H_1)}=0.5$.  The Bayesian likelihood at each $x$ is half of the likelihood with a flat prior.

We choose $H_0$ if
$$\frac{P(H_0|x)}{P(H_1|x)} = \frac{P(x|H_0)}{P(x|H_1)}\frac{P(H_0)}{P(H_1)} > 1$$
We can re-arrange the inequality to arrive at an equivalent expression
\[
\begin{aligned}
\frac{P(H_0|x)}{P(H_1|x)} = \frac{P(x|H_0)}{P(x|H_1)}\frac{P(H_0)}{P(H_1)} &> 1 \\
\frac{P(x|H_0)}{P(x|H_1)}  &> 1 \cdot \frac{P(H_1)}{P(H_0)} = c
\end{aligned}
\]

**Making Errors**  
In our previous example, we considered the situation where $c=1$ (in other words, a flat prior).  We concluded that our null hypothesis, $H_0$ (the younger child), is accepted as long as $x \geq 10$.  If the phone received 10 or more texts in the 8 hour period, then it is more likely that we have the younger child's phone.

The cdf tells us the probabilities   
$P_{younger}(X \geq 10)=`r ppois(q=9, lambda=12, lower.tail=FALSE)`$  
$P_{younger}(X < 10)=`r ppois(q=9,lambda=12)`$  
$P_{younger}(X < 10) + P_{younger}(X \geq 10) = 1$  
$P_{older}(X \geq 10)=`r ppois(q=9, lambda=8, lower.tail=FALSE)`$  
$P_{older}(X < 10)=`r ppois(q=9,lambda=8)`$  
$P_{older}(X < 10) + P_{older}(X \geq 10) = 1$  

The two possible errors are:

1) Type I: Reject $H_0$ when it is true.
2) Type II: Accept $H_0$ when it is false.

**Example**  If $c=1$, what are the probabilities of making an error in our conclusion?

Type I Error (wrongly rejecting $H_0$): If the phone had fewer than 10 texts, our model indicates that we have the older child's phone, and we make such a conclusion.  But in reality, the phone belongs to the younger child which is our $H_0$. 

To calculate the probability of this error, we use the younger child's cdf (Pois($\lambda=12$)) to find the total probability of when the younger child's phone receives fewer than 10 texts.
$$
\begin{aligned}
P(\text{Reject} \ H_0|H_0) &= P(X < 10 |H_0) \\
  &=`r ppois(9,12)`
\end{aligned}
$$

Type II Error (wrongly accepting $H_0$): If the phone had 10 or more texts, we would conclude that we had the younger child's phone.  But in reality, the phone belongs to the older child which is our $H_1$. 

To calculate the probability of this error, we use the older child's cdf (Pois($\lambda=8$)) to find the total probability of that the older child's phone receives 10 or more texts.

$$
\begin{aligned}
P(\text{Accept} \ H_0|H_1) &= P(X \geq 10 | H_1) \\
  &= `r 1-ppois(9,8)`
\end{aligned}
$$

It helps to think about this issue by telling yourself: the answer is either $H_0$ or $H_1$.  If it is $H_0$, then we are dealing with $H_0$'s cdf.  To calculate the error, we figure out what is the cumulative probability of the less likely situation.  The probability of this error has nothing to do with the actual data.  We have determined the threshold where we change from one conclusion to another.  In this case, we determined the threshold is between 9 and 10 texts.



**Example**  Now suppose $c=2$, meaning
$$\frac{P(H_0)}{P(H_1)}=\frac{1}{2}$$

So $P(H_0)=\frac{1}{3}$ and $P(H_1)=\frac{2}{3}$.

Our calculations and plots showed that the threshold shifted from 9-10 to 11-12 texts.

Type I Error (wrongly rejecting $H_0$): The phone has fewer than 12 texts, we conclude that it is the older child's phone, but we are wrong.  Use $H_0$'s (younger child) cdf:
$$
\begin{aligned}
P(\text{Reject} \ H_0|H_0) &= P(X < 12|H_0) \\
  &=0.462
\end{aligned}
$$
Type II Error (wrongly accepting $H_0$): The phone has 12 or more texts, we conclude that is the younger child's phone, but we are wrong.
$$
\begin{aligned}
P(\text{Accept} \ H_0|H_1) &= P(X \geq 12 |H_1) \\
  &= 0.112
\end{aligned}
$$

###Neyman-Pearson Paradigm  
A non-Bayesian approach to decision-making focusing on the previously described errors rather than prior probabilities.  

We have two hypotheses, the null hypothesis, $H_0$, and the alternative hypothesis, $H_1$ or $H_A$.

Review of familiar terms:

- Rejecting $H_0$ when it is true is type I error.  
- The probability of type I error is called the level of significance, and is denoted $\alpha$.
- Failing to reject (meaning, accepting) the null hypothesis when it is false is a type II error with probability $\beta$.  
- The probability of correctly rejecting $H_0$ is called power with probability $1-\beta$.
- The function of the data used to make our decision is called the test statistic.  
- The set of values of the test statistic is called the rejection region (all other values comprise the acceptance region).
- The distribution of the test statistic when $H_0$ is true is the null distribution.  

Formally, simple hypotheses are ones in which the data follow one of two possible joint distributions, each of these specified by one of these hypotheses.  

**Lemma (Neyman-Pearson)**  Suppose that $H_0$ and $H_1$ are simple hypotheses and the test that rejects $H_0$ whenever the likelihood ratio is less than $c$ has significance level $\alpha$.  Then any other test for which the significance level is less than or equal to $\alpha$ has power less than or equal to the likelihood ratio test.  

In this sense, the likelihood ratio test is most powerful.

**January 25, 2016**    
**Neyman-Pearson Proof**  
Suppose that $\mathbf{X}=(X_1, \dots X_n)$ have joint density or mass function $f(x)$ where $f$ is one of $f_0$ or $f_1$ and we test 
\[H_0:f=f_0 \ \text{vs.} \ H_1:f=f_1\]
We will choose only one of these hypotheses, so our decision function, $d(\mathbf{x})$, is actually a Bernoulli random variable taking on a value of $0$ if we accept $H_0$ and 1 otherwise.

Consider our likelihood ratio test.  We have
\[d(\mathbf{x})=
\begin{cases}
1, \ \text{Reject $H_0$:} \ f_0(\mathbf{x}) < cf_1(\mathbf{x}) \\
0, \ \text{Accept $H_0$:} \ f_0(\mathbf{x}) \geq cf_1(\mathbf{x})
\end{cases}
\]
for some positive $c$.

- Note that we have rearranged the condition of $d(\mathbf{x})=1$ from what we had already seen: At the end of the example of phones and text messages, we set up a rule that we should accept the null hypothesis if the likelihood of the null hypothesis divided by the likelihood of the alternative hypothesis exceeded a value, $c$:
\[
\frac{P(x|H_0)}{P(x|H_1)} > c \ \ \text{or} \ \ \frac{f_0(x)}{f_1(x)} > c
\]
But here we are switching our terms around and using the ratio to apply to when we reject the null hypothesis.  That is, we reject $H_0$ when $\frac{f_0(x)}{f_1(x)} < c$ and that is when $d(\mathbf{x})=1$.  So
\[
\frac{f_0(\mathbf{x})}{f_1(\mathbf{x})} < c \Rightarrow f_0(\mathbf{x}) < cf_1(\mathbf{x})
\]

The decision function is a Bernoulli random variable which means that $E[d(\mathbf{X})]=P(d(\mathbf{X})=1)$

In regard to the null hypothesis, $\alpha$ represents the probability that we reject $H_0$, or the probability that $d(\mathbf{X})=1$:
\[
E_0[d(\mathbf{X})]=P_0(d(\mathbf{X})=1)=\alpha
\]
This is the probability of type I error, whereas for the alternative hypothesis,
\[
\begin{aligned}
E_1[d(\mathbf{X})] &= P_1(d(\mathbf{X})=1) \\
  &= 1 - \beta
\end{aligned}
\]
It suffices to show that if $d^*(\mathbf{x})$ is the decision function of another test with $0 \leq d^*(\mathbf{x}) \leq 1$ and
\[
E_0[d^*(\mathbf{X})] \leq E_0[d(\mathbf{X})] = \alpha
\]
then
\[
E_1[d^*(\mathbf{X})] \leq E_1[d(\mathbf{X})]
\]
where $E_1[d(\mathbf{x})]$ is our power, so we are making a direct comparison of power.

First note that
\[
cf_1(\mathbf{x})-f_0(\mathbf{x})
\begin{cases}
  &> 0, d(\mathbf{x})=1 \\
  &\leq 0, d(\mathbf{x})=0
\end{cases}
\]
Thus,
\[
d^*(\mathbf{x})[cf_1(\mathbf{x})-f_0(\mathbf{x})] \leq d(\mathbf{x})[cf_1(\mathbf{x})-f_0(\mathbf{x})]
\]
and so,
\[
\begin{aligned}
\int \cdots \int d^*(\mathbf{x})[cf_1(\mathbf{x}) - f_0(\mathbf{x})]d\mathbf{x} &\leq \int \cdots \int d(\mathbf{x})[cf_1(\mathbf{x})-f_0(\mathbf{x})]d\mathbf{x} \\
\int \cdots \int [d^*(\mathbf{x}) \cdot cf_1(\mathbf{x}) - d^*(\mathbf{x}) \cdot f_0(\mathbf{x})]d\mathbf{x} &\leq \int \cdots \int [d(\mathbf{x}) \cdot cf_1(\mathbf{x})- d(\mathbf{x}) \cdot f_0(\mathbf{x})]d\mathbf{x} \\
\int \cdots \int [d^*(\mathbf{x}) \cdot cf_1(\mathbf{x}) - d(\mathbf{x}) \cdot cf_1(\mathbf{x})]d\mathbf{x} &\leq \int \cdots \int [d^*(\mathbf{x}) \cdot f_0(\mathbf{x}) - d(\mathbf{x}) \cdot f_0(\mathbf{x})]d\mathbf{x} \\
\int \cdots \int c[d^*(\mathbf{x}) - d(\mathbf{x})] f_1(\mathbf{x}) d\mathbf{x} &\leq \int \cdots \int [d^*(\mathbf{x}) - d(\mathbf{x})] f_0(\mathbf{x}) d\mathbf{x} \\
c\int \cdots \int [d^*(\mathbf{x})-d(\mathbf{x})]f_1(\mathbf{x})d\mathbf{x} &\leq \int \cdots \int [d^*(\mathbf{x})-d(\mathbf{x})]f_0(\mathbf{x})d\mathbf{x}
\end{aligned}
\]
Note that the above could be sums instead of integrals in the discrete case.

The left-hand side is simply $cE_1[d^*(\mathbf{x})-d(\mathbf{x})]$ or $c\Big(E_1[d^*(\mathbf{x})]-E_1[d(\mathbf{x})]\Big)$

and the right-hand side is $E_0[d^*(\mathbf{x})] - E_0[d(\mathbf{x})]$.

Since $E_0[d^*(\mathbf{x})] \leq E_0[d(\mathbf{x})]$ it follows that $E_1[d^*(\mathbf{x})] \leq E_1[d(\mathbf{x})]$

We have thus finished our proof.

The Neyman-Pearson Lemma as stated does not guarantee the existence of a most powerful test ($\alpha$-level) but merely states that the test that rejects $H_0$ for $\frac{f_0(\mathbf{x})}{f_1(\mathbf{x})}<c$ will be a most powerful test for some level $\alpha$ (but not necessarily for all $\alpha$).

Moreover, the Neyman-Pearson Lemma does not guarantee the uniqueness a most powerful test.  In fact, there may be many test functions having the same power as the most powerful test function under the Neyman-Pearson Lemma.  

**Example**  A random sample of size $n$ drawn from an exponential population is used to test the null hypothesis $\theta=\theta_0$ against the alternative hypothesis $\theta=\theta_1>\theta_0$.  Find the most powerful critical region of size $\alpha$.  

**Solution**  The likelihood ratio is
\[
\begin{aligned}
T(\mathbf{X}) &= \frac{f_0(\mathbf{X})}{f_1(\mathbf{X})} \\
  &= \frac{f_0(X_1)f_0(X_2)\dots f_0(X_n)}{f_1(X_1)f_1(X_2)\dots f_1(X_n)} \\
  &= \frac{\frac{1}{\theta_0^n}\exp\bigg[-\frac{1}{\theta_0}\sum_{i=1}^n X_i \bigg]}{\frac{1}{\theta_1^n}\exp\bigg[-\frac{1}{\theta_1}\sum_{i=1}^n X_i \bigg]} \\
  &= \bigg(\frac{\theta_1}{\theta_0}\bigg)^n\exp\bigg[-\frac{1}{\theta_0}\bigg(\sum_{i=1}^n X_i\bigg) -\frac{1}{\theta_1}\sum_{i=1}^n X_i \bigg] \\
  &= \bigg(\frac{\theta_1}{\theta_0}\bigg)^n\exp\bigg[-\bigg(\frac{1}{\theta_0}-\frac{1}{\theta_1}\bigg)\sum_{i=1}^n X_i\bigg]
\end{aligned}
\]

We are interested in setting a threshold with which we would reject $H_0$ if the likelihood ratio descends below a certain point.

But note that the likelihood ratio is monotonically decreasing as $\sum_{i=1}^n X_i$ increases (we know this because we are given $\theta_1 > \theta_0$).  The parameters $\theta_0$, $\theta_1$, and $n$ are fixed.  The only variable is $\sum_{i=1}^n X_i$.  Therefore, instead of setting a threshold for the likelihood ratio, we will do so for $\sum_{i=1}^n X_i$.  We will reject $H_0$ if 
\[\sum_{i=1}^n X_i > k\]

The reason why we focus on $\sum_{i=1}^n X_i$ and not the likelihood ratio is because we know that the sum of exponential i.i.d. random variables is a gamma-distributed random variable.

Thus, if $H_0$ is true, then $\sum_{i=1}^n X_i \sim \text{Gamma}(n, \theta_0)$. 

A gamma distribution looks something like

```{r echo=FALSE, fig.height=3, fig.width=6}
library(ggplot2)
ggplot(data.frame(x=c(0, 250), y=c(0, .5)), aes(x)) + 
  stat_function(fun=function(x) dgamma(x=x, shape=10, scale=10)) +
  geom_vline(data=NULL, aes(xintercept=175),
               linetype="solid", size=.5, colour="black") +
  labs(title=expression(paste("Distribution of ", Sigma, "X")))
```

The vertical line represents a hypothetical location of $k$ and is interpreted in much the same way as one interprets a p-value with respect to a rejection region that is specified for a normal distribution.  

For a specified $\alpha$, $k$ is the $(1-\alpha)^{th}$ quantile of this gamma distribution, say $\pi_{\alpha}$.


The Neyman-Pearson Lemma tells us that the most powerful test rejects $H_0$ for $\sum_{i=1}^n X_i > k$, where $k=\pi_{\alpha}$, when 
\[\alpha=P_{\theta_0}\bigg[\sum_{i=1}^n X_i > k\bigg]\]
and the power of the test is 
\[P_{\theta_1}\bigg[\sum_{i=1}^n X_i > k\bigg]\]

Elsewhere the notation appears as
\[
\text{Rejection Region:} \ \alpha=P\bigg(\sum_{i=1}^n X_i > k \ \Big| \ \theta_0 \bigg)
\]
\[
\text{Power:} \ P\bigg(\sum_{i=1}^n X_i > k \ \Big| \ \theta_1 \bigg)
\]

(Put in simple terms, the Neyman-Pearson Lemma tells us that to find the most powerful test, find $k$ such that the rejection region created by $k$ is equal to $\alpha$.)

**Example** Using the findings from above, let $\alpha=0.05$ and $n=20$.  Our hypotheses:
\[
H_0: \theta_0 = 3 \ \ \ \text{vs.} \ \ \ H_1: \theta_1 = 10
\]
We are given $\alpha$, now we must determine $k$.
\[
0.05=P\bigg(\sum_{i=1}^{20} X_i > k \ \Big| \ \theta_0=3 \bigg)
\]
How to determine $k$ in `R`: `qgamma(p=0.95, shape=20, scale=3)` = `r qgamma(p=0.95, shape=20, scale=3)`
```{r fig.height=4, fig.width=5}
k <- qgamma(p=0.95, shape=20, scale=3) # gives us k
1-pgamma(q=k, shape=20, scale=10) # power of test
sum.x <- 1:300
H_0 <- dgamma(x=sum.x, shape=20, scale=3) # null hypothesis
H_1 <- dgamma(x=sum.x, shape=20, scale=10) # alt hypothesis
plot(x=sum.x, y=H_0, xlab="Sum of X Values", ylab="Density", type='l')
lines(x=sum.x, y=H_1, col='red')
# represents critical value
abline(v=k, col='blue')
# alpha is area under black curve that is to the right of the vertical line
# power is the area to the right of the blue line under the red curve
```

Some variables that affect power:

- The significance level chosen ($\alpha$).
- The sample size, $n$.  Increasing the sample size will condense the curves so that the range is narrower and taller.
- The difference between the two hypotheses, $\theta_0$ and $\theta_1$.  

The power in this example is extremely large due to the difference in hypotheses.

If we set $H_1: \theta_1=5$
```{r fig.height=4, fig.width=5}
new.H_1 <- dgamma(x=sum.x, shape=20, scale=5)
plot(x=sum.x, y=H_0, type='l')
lines(x=sum.x, y=new.H_1, col='green')
1-pgamma(q=k, shape=20, scale=5) # power of test with new alt hypothesis
```

```{r eval=FALSE, echo=FALSE}
library(ggplot2)
ggplot(data.frame(x = c(-5, 5)), aes(x)) + stat_function(fun = dnorm)
x <- seq(from=0,to=1.5,by=0.01) 
y <- dnorm(x)
df <- data.frame(x=x, y=y)
ggplot(data.frame(x = c(-5, 5)), aes(x)) +
  #geom_vline(data=NULL, aes(xintercept=0.5),
  #             linetype="solid", size=.5, colour="black") +
  geom_segment(mapping=aes(x=0.5,y=0,xend=0.5,yend=.35), size=.5) +
  layer(geom = "area", data=df, mapping = aes(x = ifelse(x>0.5 & x<1.5, x, 0)),
          geom_params = list(fill="red", alpha = 0.5)) +
  stat_function(fun = dnorm, args = list(mean = 0, sd = 1)) +
  scale_y_continuous(limits = c(0, max(df$y)))


x <- seq(from=0.5,to=1.5,by=0.01) 
y <- dnorm(x)
df <- data.frame(x=x, y=y)
ggplot(data.frame(x = c(-5, 5)), aes(x)) + 
  stat_function(fun = dnorm, args = list(mean = 0, sd = 1)) +
  geom_segment(aes(x=0.5,y=0,xend=0.5,yend=.35)) +
  geom_polygon(mapping=aes(x, y), data=df)

###
ids <- factor(c("1.1", "2.1", "1.2", "2.2", "1.3", "2.3"))
values <- data.frame(
  id = ids,
  value = c(3, 3.1, 3.1, 3.2, 3.15, 3.5)
)
positions <- data.frame(
  id = rep(ids, each = 4),
  x = c(2, 1, 1.1, 2.2, 1, 0, 0.3, 1.1, 2.2, 1.1, 1.2, 2.5, 1.1, 0.3,
  0.5, 1.2, 2.5, 1.2, 1.3, 2.7, 1.2, 0.5, 0.6, 1.3),
  y = c(-0.5, 0, 1, 0.5, 0, 0.5, 1.5, 1, 0.5, 1, 2.1, 1.7, 1, 1.5,
  2.2, 2.1, 1.7, 2.1, 3.2, 2.8, 2.1, 2.2, 3.3, 3.2)
)
# Currently we need to manually merge the two together
datapoly <- merge(values, positions, by=c("id"))
###


x<-seq(0.0,0.1699,0.0001)  
ytop<-dnorm(0.12,0.08,0.02)
MyDF<-data.frame(x=x,y=dnorm(x,0.08,0.02))
shade <- rbind(c(0.12,0), subset(MyDF, x > 0.12), c(MyDF[nrow(MyDF), "X"], 0))
p <- qplot(x=MyDF$x,y=MyDF$y,geom="line") 
p + geom_segment(aes(x=0.12,y=0,xend=0.12,yend=ytop)) +
    geom_polygon(data = shade, aes(x, y))


## Create data
dat <- with(density(rnorm(100)), data.frame(x, y))
## Color the area under the curve between -1.2 and 1.1
ggplot(data = dat, mapping = aes(x = x, y = y)) +
    layer(geom = "line") +
    layer(geom = "area", mapping = aes(x = ifelse(x>-1.2 & x<1.1 , x, 0)),
          geom_params = list(fill = "red", alpha = 0.5)) +
    scale_y_continuous(limits = c(0, max(dat$y)))


#' Draw Normal Distribution Density with an area shaded in.
#'
#' @param lb Lower bound of the shaded area. Use \code{-Inf} for a left tail.
#' @param ub Upper bound of the shaded area. Use \code{Inf} for a right tail.
#' @param mean Mean of the normal distribution
#' @param sd Standard deviation of the normal distribution
#' @param limits Lower and upper bounds on the x-axis of the area displayed.
#' @return ggplot object.
#' @examples
#' # Standard normal with upper 2.5% tail shaded
#' normal_prob_area_plot(2, Inf)
#' # Standard normal with lower 2.5% tail shaded
#' normal_prob_area_plot(-Inf, 2)
#' # standard normal with middle 68% shaded.
#' normal_prob_area_plot(-1, 1)
normal_prob_area_plot <- function(lb, ub, mean = 0, sd = 1, limits = c(mean - 3 * sd, mean + 3 * sd)) {
    x <- seq(limits[1], limits[2], length.out = 100)
    xmin <- max(lb, limits[1])
    xmax <- min(ub, limits[2])
    areax <- seq(xmin, xmax, length.out = 100)
    area <- data.frame(x = areax, ymin = 0, ymax = dnorm(areax, mean = mean, sd = sd))
    (ggplot()
     + geom_line(data.frame(x = x, y = dnorm(x, mean = mean, sd = sd)),
                 mapping = aes(x = x, y = y))
     + geom_ribbon(data = area, mapping = aes(x = x, ymin = ymin, ymax = ymax))
     + scale_x_continuous(limits = limits))
}

normal_prob_area_plot(2, Inf)


#stumbled upon, try using with airline data
#b <- ggplot(mtcars, aes(wt, mpg)) + geom_point()
#df <- data.frame(x1 = 2.62, x2 = 3.57, y1 = 21.0, y2 = 15.0)
#b + geom_curve(aes(x = x1, y = y1, xend = x2, yend = y2, colour = "curve"), data = df, #curvature = -0.2) + geom_segment(aes(x = x1, y = y1, xend = x2, yend = y2, colour = "segment"), data = df)
```



**January 27, 2016**  
**Problem**  Let $H_0: X_1,\dots, X_8 \sim \text{Poisson}(\lambda=2) \ \text{vs.} \ H_1: X_1,\dots ,X_8 \sim \text{Poisson}(\lambda=\frac{1}{2})$. Find the most powerful test.  
**Solution**  
\[
\begin{aligned}
\Lambda &= \frac{L(\lambda_0)}{L(\lambda_1)} \\
  &= \frac{2^{\sum X_i}e^{-8(2)}}{\frac{1}{2}^{\sum X_i}e^{-12}} \\
  &= 4^{\sum X_i}e^{-12}
\end{aligned}
\]
The likelihood ratio is a monotonically increasing function of $\sum X_i$ as $\sum X_i$ increases.  That is to say that $H_0$ becomes increasingly probable as $\sum X_i$ increases.  So if the ratio is low, we are inclined to reject the null hypothesis.

The $\alpha$-level most powerful test rejects $H_0$ when $\sum_{i=1}^8 X_i < k$, where 
\[P\bigg(\sum_{i=1}^8 X_i < k \ \Big| \ \lambda_0=2\bigg)=\alpha\]

What is the above equation saying?  We want the sum of $X_i$ variables to be less than a specified $k$ such that the probability is equal to $\alpha$.

In this example, if given $\alpha$, we can find $k$ since the sum of Poisson random variables are also Poisson distributed where the new parameter is $n\lambda$. So $\sum_{i=1}^8 X_i \sim \text{Pois}(8\lambda)$.

Let $\alpha=0.05$.  We had 8 samples, so our new parameter is $n\lambda = 16$.  We will find $k$ through trial and error.  Try $k=6$ (cumulative probability for 5 or fewer):
```{r}
ppois(q=5, lambda=16)
```
So the cumulative probability for total occurrences equaling 5 and below is `r ppois(q=5, lambda=16)`.  This is too small for $\alpha$.

We can raise $\sum X_i$ to expand the rejection region so that $\alpha=0.05$.
```{r}
ppois(q=10, lambda=16)
```
$P(\sum X_i \leq 10)$ is too large for our $\alpha$ target.

```{r}
ppois(q=9, lambda=16)
```
Since Poisson deals with discrete values, this is the closest that we can have to satisfy our criteria $\alpha=0.05$ without exceeding our threshold.

So when $k=10$, then $\alpha=0.0433$.

Then our power is the probability that we will correctly reject the null hypothesis if the alternative hypothesis is true: 
\[P\bigg(\sum_{i=1}^8 \leq 9 \ \Big| \ \lambda_1=\frac{1}{2}\bigg)\]

New parameter of alternative hypothesis: $n\lambda=4$ 
```{r}
ppois(q=9, lambda=4)
```
So our power is $0.992$.  The large power is due to the large difference between the two parameters.

###General Form of Neyman-Pearson Lemma    
This method is to maximize $E_1[d(\mathbf{X})]$ subject to the constraints
\[
E_0[d(\mathbf{X})]=\alpha \ \text{and} \ 0 \leq d(\mathbf{X}) \leq 1
\]
The optimal $d$ is given by
\[
d(\mathbf{X}) =
\begin{cases}
1, f_0(\mathbf{X}) < cf_1(\mathbf{X}) \\
k, f_0(\mathbf{X}) = cf_1(\mathbf{X}) \\
0, f_0(\mathbf{X}) > cf_1(\mathbf{X}) 
\end{cases}
\]
where $c$ and $k \in (0,1)$ are chosen so that the constraints are satisfied.  In the case where the statistic $T(\mathbf{X})=f_0(\mathbf{X})/f_1(\mathbf{X})$ is a continuous random variable (which implies $X_1, \dots, X_n$ are continuous), we can take the optimal $d$ to be $0$ or $1$ for all possible values of $X_1, \dots, X_n$ and so a most powerful (MP) test of $H_0: f=f_0$ vs. $H_1: f=f_1$ exists for all $\alpha > 0$.  

However, for a given level $\alpha$, a most powerful test $H_0$ vs. $H_1$ need not exist if $T(\mathbf{X})$ discrete.  

**Example**  Suppose that $X_1, \dots, X_n$ are i.i.d. uniform random variables on the interval $[0, \theta]$ where $\theta$ is either $\theta_0$ or $\theta_1$ (where $\theta_0 > \theta_1$).  We want to test $H_0: \theta = \theta_0$ vs. $H_1: \theta=\theta_1$ at level $\alpha$.  

```{r fig.width=6, fig.height=2, echo=FALSE}
ggplot(data.frame(x=c(0, 5), y=c(0,1.25)), aes(x=x, y=y)) + geom_segment(mapping=aes(x=0,y=1,xend=1,yend=1), size=.5, colour="blue") + 
  geom_segment(mapping=aes(x=0,y=.5,xend=2,yend=.5), size=.5, colour="red") + 
  geom_segment(mapping=aes(x=0,y=0,xend=0,yend=1), size=.5) + geom_segment(mapping=aes(x=1,y=0,xend=1,yend=1), size=.5, colour="blue") + 
  geom_segment(mapping=aes(x=2,y=0,xend=2,yend=.5), size=.5, colour="red") + 
  scale_x_continuous(breaks = c(1, 2), labels=c(expression(theta["1"]), expression(theta["0"]))) +
  coord_cartesian(xlim=c(0, 2.25), ylim=c(0,1.25)) + 
  labs(title=expression(paste("PDFs of 2 Distributions, ", theta["1"], "<", theta["0"])))
```


The pdf of the uniform is $f(x)=\frac{1}{b-a}$, and the joint density of $\mathbf{X}=(X_1,\dots,X_n)$ is 
\[
f(\mathbf{X};\theta)=\frac{1}{\theta^n} \mathbf{I}_{(max(X_1,\dots,X_n)\leq \theta)}
\]

- The purpose of $\mathbf{I}_{(max(X_1,\dots,X_n)\leq \theta)}$ is to take into account the possibility of the event $\theta$.  If $(max(X_1,\dots,X_n) > \theta)$, the $\theta$ cannot be our parameter for the upper bound.  Then $\mathbf{I}=0$ and the joint probability of $f(\mathbf{X})$ and $\theta$ is zero.

A most powerful test of $H_0$ vs. $H_1$ will be based on the test statistic $\Big[$ and let $\text{max}(X_1, \dots, X_n) \equiv X_{(n)}$ (the notation $X_{(n)}$ comes from order statistics $\Big]$
\[
\begin{aligned}
T(\mathbf{X}) &= \frac{f(\mathbf{X};\theta_0)}{f(\mathbf{X}; \theta_1)} \\
  &= \frac{\frac{1}{\theta_0-0}\cdots\frac{1}{\theta_0-0}}{\frac{1}{\theta_1-0}\cdots \frac{1}{\theta_1-0}} \\
  &= \bigg(\frac{\theta_1}{\theta_0}\bigg)^n \mathbf{I}_{(X_{(n)}\leq \theta_1)}
\end{aligned}
\]

- The indicator variable is set to compare the largest $X_i$ against the smaller upper bound, $\theta_1$.  If $X_{(n)} > \theta_1$, then we can  conclude that $H_A: \theta = \theta_1$ couldn't be true.

Note that there are two possibilities for $T$:

- $\Big(\frac{\theta_1}{\theta_0}\Big)^n$ if $X_{(n)}$ is equal to or less than $\theta_1$
- $0$ if $X_{(n)}$ is greater than $\theta_1$.

It follows that the test that rejects $H_0$ when $X_{(n)} \leq \theta_1$ will be a most powerful test of $H_0$ vs. $H_1$ with level
\[
\begin{aligned}
\alpha &= P_{\theta_0}(X_{(n)} \leq \theta_1) \\
  &= \bigg(\frac{\theta_1}{\theta_0}\bigg)^n
\end{aligned}
\]
Think about why this is the probability: if $X_i$ is uniformly distributed, the probability that it will fall within the larger bound is 100%.  The probability that it will fall within the lower bound is $\theta_1 / \theta_0$.

Recall $\theta_1 < \theta_0$ so $\frac{\theta_1}{\theta_0} < 1$. Raising $\frac{\theta_1}{\theta_0}$ to the $n^{th}$ power can lower the value to equal $\alpha$.   So $n$ can be manipulated to meet $\alpha$.  Otherwise, we have no way to create a scenario to set the $\alpha$ value - $\theta_1$ and $\theta_0$ are fixed values.

The power of the test is
\[
P_{\theta_1}(X_{(n)} \leq \theta_1) = 1
\]
This is telling us an obvious fact: if $\theta_1$ (the smaller one) is the true parameter, then the maximum value, $X_{(n)}$, must be less than the true upper bound, $\theta_1$.  

Note that this test will also be the most powerful test for any level $\alpha > \Big(\frac{\theta_1}{\theta_0}\Big)^n$ since the power will always be 1.

If we want the most powerful test for $\alpha < \Big(\frac{\theta_1}{\theta_0}\Big)^n$ in the previous example, the Neyman-Pearson Lemma does not help directly.  

However, since $X_{(n)}$ is sufficient for $\theta$, we intuitively should reject $H_0$ for $X_{(n)} \leq k$ where
\[
P_{\theta_0}(X_{(n)} \leq k) = \bigg(\frac{k}{\theta_0}\bigg)^n = \alpha
\]
```{r fig.width=6, fig.height=2, echo=FALSE}
ggplot(data.frame(x=c(0, 5), y=c(0,1.25)), aes(x=x, y=y)) + geom_segment(mapping=aes(x=0,y=1,xend=1,yend=1), size=.5, colour="blue") + 
  geom_segment(mapping=aes(x=0,y=.5,xend=2,yend=.5), size=.5, colour="red") + 
  geom_segment(mapping=aes(x=0,y=0,xend=0,yend=1), size=.5) + geom_segment(mapping=aes(x=1,y=0,xend=1,yend=1), size=.5, colour="blue") + 
  geom_segment(mapping=aes(x=2,y=0,xend=2,yend=.5), size=.5, colour="red") + 
  geom_segment(mapping=aes(x=.1,y=0,xend=.1,yend=.5), size=.1) + 
  scale_x_continuous(breaks = c(.1, 1, 2), labels=c(expression(alpha), expression(theta["1"]), expression(theta["0"]))) +
  coord_cartesian(xlim=c(0, 2.25), ylim=c(0,1.25)) + 
  labs(title=expression(paste("PDFs of 2 Distributions, ", theta["1"], "<", theta["0"])))
```

Solving for $k$, we get $k=\theta_0\alpha^{1/n}$ and so the power of the test is
\[
\begin{aligned}
P_{\theta_1}(X_{(n)} \leq k) &= \bigg(\frac{k}{\theta_1}\bigg)^n \\
P_{\theta_1}(X_{(n)}\leq \theta_0\alpha^{1/n}) &= \bigg(\frac{\theta_0\alpha^{1/n}}{\theta_1}\bigg)^n \\
  &=\alpha\bigg(\frac{\theta_0}{\theta_1}\bigg)^n
\end{aligned}
\]
To show this is a most powerful test for $\alpha < \Big(\frac{\theta_1}{\theta_0}\Big)^n$, we use the general form of the Neyman-Pearson Lemma, a function $d$ that maximizes $E_{\theta_0}[d(\mathbf{X})]$ under the constraints
\[
E_{\theta_0}[d(\mathbf{X})] = \alpha < \Big(\frac{\theta_1}{\theta_0}\Big)^n \ \text{and} \ 0 \leq d(\mathbf{X}) \leq 1
\]
is 
\[
d(\mathbf{X})=
\begin{cases} 
1, X_{(n)} \leq \theta_0 \alpha^{1/n} \\
0, \text{otherwise}
\end{cases}
\]

Since $E_{\theta_1}[d(\mathbf{X})]=\alpha\bigg(\frac{\theta_0}{\theta_1}\bigg)^n=P_{\theta_1}[X_{(n)}\leq \theta_0\alpha^{1/n}]$, the test that rejects $H_0$ if $X_{(n)} \leq \theta_0\alpha^{1/n}$ is a most powerful test for level $\alpha < \bigg(\frac{\theta_1}{\theta_0}\bigg)^n$

The p-value is defined as the smallest level of significance at which an experimenter using a particular test statistic, $T$, would reject $H_0$ on the basis of the observed outcome.  

###Uniformly Most Powerful Tests  
If a hypothesis does not completely specify the probability distribution, then the hypothesis is called a composite hypothesis.

In some cases, the theory of the Neyman-Pearson Lemma can be extended to cover composite hypotheses.  

If the alternative, $H_1$, is composite, a test that is most powerful for every simple alternative in $H_1$ is said to be uniformly most powerful.

**Definition**  Formally, a level-$\alpha$ test, $\psi$, is uniformly most powerful (UMP) for $H_0: \theta \in \Theta_0$ vs. $H_1: \theta \in \Theta_1$, for any other level-$\alpha$ test $\psi^*$.  

**February 01, 2016**  
**Example** Let $Y$ have a binomial distribution bin$(n,p)$.  Find a uniformly most powerful test of the simple null hypothesis $H_0:p=p_0$ against the one-sided composite alternative hypothesis $H_1: p=p_1>p_0$.

We will set up a likelihood ratio and reject $H_0$ if the ratio is equal to or below a specified level $c$:
\[
T(y) =\frac{f_0(y)}{f_1(y)} = \frac{\binom{n}{y}p_0^y(1-p_0)^{n-y}}{\binom{n}{y}p_1^y(1-p_1)^{n-y}} \leq c
\]
As in previous examples, we want to re-arrange this equality/inequality so that we can isolate the random variable whose distribution we know. And we know $y$ is binomial.

The ratio is equivalent to 
\[
T(y) = \bigg(\frac{p_0(1-p_1)}{p_1(1-p_0)}\bigg)^y\bigg(\frac{1-p_0}{1-p_1}\bigg)^n \leq c
\]
Take the log of both sides
\[
y\log\bigg(\frac{p_0(1-p_1)}{p_1(1-p_0)}\bigg) \leq \log c - n \log\bigg(\frac{1-p_0}{1-p_1}\bigg)
\]
Since we are given $p_0 < p_1$, we have $p_0(1-p_1) < p_1(1-p_0)$, and it follows that $\log\Big(\frac{p_0(1-p_1)}{p_1(1-p_0)}\Big) < 0$.  

Then when we divide both sides by $\log\Big(\frac{p_0(1-p_1)}{p_1(1-p_0)}\Big)$, the inequality switches direction:
\[
y \geq \frac{\log c - n \log \bigg(\frac{1-p_0}{1-p_1}\bigg)}{\log \bigg(\frac{p_0(1-p_1)}{p_1(1-p_0)}\bigg)}
\]
for each $p_1 > p_0$.  The right-hand side of the inequality consist of constant terms: $c,n,p_0,p_1$.  So we can simply equate that to a constant that we'll call $k$:
\[
y \geq k
\]

To find our uniformly most powerful test, select $k$ such that $P_0(Y\leq k)=\alpha$.

Note that because this is a one sided test (i.e. $p_1 > p_0$), the same value of $k$ can be used for every $p_1 > p_0$ (doesn't depend on the value $p_1$).

Since the rejection region defines a test that is most powerful against each simple alternative $p_1 > p_0$, this is a uniformly most powerful test.

- Typically a uniformly most powerful test does not exist for two-sided tests.  If this were a two-sided test (i.e. $p_0 \neq p_1$), then $k$ would depend on $p_1$: $k$ would be one value if $p_0 < p_1$ and another value if $p_0 > p_1$.  This is because of the term $\log\Big(\frac{p_0(1-p_1)}{p_1(1-p_0)}\Big)$ which is negative when $p_0 < p_1$ and positive when $p_0 > p_1$.  So if $H_1$ was $p_0 \neq p_1$, then we could not propose a single test that would be most powerful against all possible alternatives.


To put this example in more concrete terms, suppose  
$\alpha=0.05$  
$n=40$  
$p_0=0.5$  
To determine our threshold $k$:
\[
\begin{aligned}
P(Y \leq k \ \big| \ p=p_0) &= \alpha \\
P(Y \leq k \ \big| \ p=0.5) &= 0.05 \\
\end{aligned}
\]
Use trial and error to determine $k$.  To get an idea of the distribution of $Y$ under $H_0$:
```{r echo=FALSE, fig.width=5, fig.height=4}
x <- 0:40
plot(x=x, y=dbinom(x=x, size=40, prob=.5), type='h')
```

Our rejection region, $\alpha$, will be the cumulative probability equal to or above $k$.  At $\alpha=0.05$, we can try $k=24$: 

`1 - pbinom(q=23, size=40, prob=.5)`$=$ `r 1-pbinom(q=23, size=40, prob=.5)`.

Too much.  

- Note that `pbinom(q=23, size=40, prob=.5)` calculates the cumulative probability of $k=23$ and below.  So `1 - pbinom(q=23, size=40, prob=.5)` calculates the complement: $k=24$ and above.

Try $k=26$: `1 - pbinom(q=25, size=40, prob=.5)` $=$ `r 1 - pbinom(25, 40, 0.5)`.

A little too low for our $\alpha=0.05$, but the best option given that we are dealing with discrete values.

Just to be sure, $k=25$: `1 - pbinom(q=24, size=40, prob=.5)` = `r 1 - pbinom(24, 40, 0.5)`

In conclusion, we would reject $H_0$ for any value of $y \geq 26$,

For the power of the test as a function of $p$, we calculate right-tail cumulative probabilities for $y \geq 26$.
```{r fig.width=5, fig.height=4}
p1 <- seq(0.5, 1, length=1000) # for H1
plot(x=p1, y=1-pbinom(q=25, size=50, prob=p1), type='l')
```

Note that our plot for power does not include $p_1 < 0.5$ since that wasn't part of our alternative hypothesis.


###Duality of Hypothesis Tests and Confidence Regions  
In decision theory, developing confidence regions and performing hypothesis test are "two sides of the same coin."

Generally speaking, confidence regions are used in estimation to find a region where we are confident of where the unknown parameters lie.

On the other hand, hypothesis testing is generally used to determine if a parameter is a particular value or not.  

The intuition is that if the value under consideration in the hypothesis test lies in the confidence region, then we should accept (i.e. fail to reject) $H_0$, whereas if the value falls outside the confidence region, we should accept $H_1$.

**Example**  Let $X_1, \dots, X_n$ be i.i.d. normal with unknown mean $\mu$ and known variance $\sigma^2$.  We know that the optimal $(1-\alpha)\times 100\%$ confidence interval for $\mu$ is
\[
\overline{X} \pm z_{\alpha/2}\frac{\sigma}{\sqrt{n}}
\]
(Reminder: for unknown $\mu$ and known $\sigma$, use $z$-distribution; for unknown $\mu$ and unknown $\sigma$, use $t$-distribution.)

For the test $H_0: \mu=\mu_0$  vs. $H_1: \mu\neq \mu_0$, it can be shown that the generalized likelihood ratio test (shown in Example A, 9.4) accepts $H_0$ for small values of $|\overline{X} - \mu_0|$, specifically if $|\overline{X} - \mu_0| < z_{\alpha/2}\frac{\sigma}{\sqrt{n}}$, or if
\[
\overline{X} - z_{\alpha/2}\frac{\sigma}{\sqrt{n}} < \mu_0 < \overline{X} + z_{\alpha/2}\frac{\sigma}{\sqrt{n}}
\]
From the book: Comparing the aceptance region of the test to the confidence interval, we see that $\mu_0$ lies in the confidence interval for $\mu$ iff the hypothesis test  accepts.  In other words, *the confidence interval consists precisely of all those values of $\mu_0$ for which the null hypothesis $H_0: \mu=\mu_0$ is accepted.*  

This example shows the duality of two-sided confidence intervals and two-sided test.  The same holds for one-sided confidence intervals and one-sided tests.

We now see this holds more generally.  

Theorem A will show that we can use the acceptance region of a hypothesis test to determine a confidence interval.

Theorem B will show the converse of A: we can use a confidence interval to determine an acceptance region for a hypothesis test.

**Theorem A**  Suppose that for every value $\theta_0 \in \Theta$, there is a test at level $\alpha$ of the hypothesis $H_0: \theta=\theta_0$.  Denote the acceptance region of the test by $A(\theta_0)$.  Then the set 
\[
C(\mathbf{X})=\{\theta: \mathbf{X} \in A(\theta_0)\}
\]
is a $(1-\alpha)\times 100\%$ confidence region for $\theta$. 

**Proof**  Since $A$ is the acceptance region of a test at level $\alpha$,
\[
P(\mathbf{X} \in A(\theta_0) \ | \ \theta=\theta_0) = 1 - \alpha
\]
Now
\[
\begin{aligned}
P(\theta_0 \in C(\mathbf{X}) \ | \ \theta=\theta_0) &= P(\mathbf{X} \in A(\theta_0) \ | \ \theta=\theta_0) \\
 &= 1 - \alpha
\end{aligned}
\]
by definition of $C(\mathbf{X})$. 

- In other words, a $(1-\alpha)\times 100\%$ confidence region for $\theta$ consists of all those values of $\theta_0$ for which the hypothesis that $\theta=\theta_0$ will not be rejected at level $\alpha$.  

**Theorem B**  Suppose that $C(\mathbf{X})$ is a $(1-\alpha)\times 100\%$ confidence region for $\theta$; that is, for every $\theta$,
\[
P(\theta_0 \in C(\mathbf{X}) \ | \ \theta=\theta_0) = 1 - \alpha
\]
Then an acceptance region for a test at level $\alpha$ of the hypothesis $H_0: \theta=\theta_0$ is
\[
A(\theta_0) = \{\mathbf{X}: \theta_0 \in C(\mathbf{X})\}
\]
**Proof** The test has level $\alpha$ because 
\[
P(\mathbf{X} \in A(\theta_0) \ | \ \theta=\theta_0) = P(\theta_0 \in C(\mathbf{X}) \ | \ \theta=\theta_0) = 1- \alpha
\]

- In other words, Theorem B states that the hypothesis that $\theta=\theta_0$ is accepted if $\theta_0$ lies in the confidence region.  

The usefulness of these theorems lies in the realization that in a given situation, it may by easier to derive an acceptance region rather than construct a confidence region (or vice versa).

Note: This is a means to derive **a** confidence region from a test (Theorem A) or **an** acceptance region from a confidence region (Theorem B).  

###Uniformly Most Accurate Confidence Bounds  
Within a certain notion of accuracy of confidence bounds, which is connected to the power of the associated one-sided test, optimality of the test translates into accuracy of the bounds.  

- Theorems A & B showed the connection between acceptance region and confidence interval.  Now we'll show that a uniformly most powerful test (which pertains to acceptance regions, almost always one-sided) will give us a uniformly most accurate confidence bound (which pertains to confidence intervals, also one-sided).

Consider, at the $(1-\alpha)$ level, two possible lower confidence bounds for $\theta$: $\theta_l$ and $\theta_l^*$.  The fact that they are supposed to be *lower* confidence bounds means that they *should* fall below the true $\theta$.

But it's not good enough that $\theta_l$ and $\theta_l^*$ are simply below the true $\theta$.  We also want the bounds to be close to $\theta$.  Thus, we say that the bound with the smaller probability of being far below $\theta$ is more accurate.  (Closer is more accurate).

**Definition** A level $(1-\alpha)$ lower confidence bound (LCB) $\theta_l^*$ of $\theta$ is said to be more accurate than a competing $(1-\alpha)$ LCB $\theta_l^*$ iff, for any fixed $\theta$ and all $\theta' < \theta$,
\[
\begin{aligned}
P_{\theta}(\theta_l^*(X) \leq \theta') \leq P_{\theta}(\theta_l(X) \leq \theta') && (\dagger)
\end{aligned}
\]
(This equation will be referenced a few times below.)  

Of course, there is an analogous definition for upper bounds.  

Lower confidence bounds $\theta^*_l$ satisfying $(\dagger)$ for all competitors is called **uniformly most accurate**.  

Note: $\theta_l^*$ is a uniformly most accurate level $(1-\alpha)$ LCB for $\theta$ iff $-\theta^*_l$ is a uniformly most accurate level $(1-\alpha)$ UCB for $-\theta$.  

**Example**  Suppose $X_1, \dots, X_n$ are i.i.d normal with unknown mean, $\mu$, and known variance $\sigma^2$.  

As seen numerous times, the level $\alpha$ test for $H_0: \mu=\mu_0$ vs. $H_1: \mu > \mu_0$ rejects $H_0$ when 
\[
\frac{\overline{X}-\mu_0}{\frac{\sigma}{\sqrt{n}}} \geq z_{(1-\alpha)}
\]

Using our familiar method for finding confidence intervals, we know that the corresponding lower confidence bound is 
\[
\begin{aligned}
\overline{X}-z_{1-\alpha}\frac{\sigma}{\sqrt{n}} && (\mu_l^*)
\end{aligned}
\]  
(Understand the procedure: when the proposed alternative is greater than the null, we find the lower confidence bound.)

It can be shown that $\mu^*_l$ is uniformly most accurate (shown in the next theorem).

As it turns out, $(\dagger)$ is nothing more than a comparison of power functions.  

**Theorem**  Let $\theta_l^*$ be a level $(1-\alpha)$ LCB for $\theta$ such that for each $\theta_0$ the associated test whose critical function $d(x, \theta_0)$ is given by 
\[
d(x, \theta_0) = 
\begin{cases}
1,& \ \text{if $\theta_l^*(x) > \theta_0$} \\
0, & \ \text{otherwise}
\end{cases}
\]
is uniformly most powerful level $\alpha$ for $H_0: \theta=\theta_0$ vs. $H_1: \theta> \theta_0$.  Then $\theta^*_l$ is uniformly most accurate at level $(1-\alpha)$.  
**Proof**  Omitted by professor

**February 03, 2016**  
We now look at general methods and results when using the likelihood ratio test.  

A couple of notes on the power of the likelihood ratio test  

1) Generally not optimal in situations where the hypotheses are not simple; however the cases where it is not optimal tend to be the cses where no optimal test exists (by optimal we mean power).

2) Up to now, we have been looking at one parameter models; these type of optimal tests do not exist for models with more than one parameter.  

The general framework is similar to what we saw before.  We have observations $\mathbf{X} = (X_1, \dots X_n)$ with joint density or mass function (aka frequency function) $f(\mathbf{x}|\theta)$ where $\theta \in \Theta$ with $\Theta = \Theta_0 \cup \Theta_1$; and we wish to test $H_0: \theta \in \Theta_0$ vs. $H_1:\theta \in \Theta_1$.

In the case of a composite hypothesis, we do this by evaluating each likelihood at the value $\theta$ that maximizes it yielding the generalized likelihood ratio
\[
\Lambda^* =\frac{\max_{\theta \in \Theta_0}L(\theta)}{\max_{\theta \in \Theta_1} L(\theta)}
\]
for which small values favor $H_1$.

Typically, we rather use the statistic
\[
\Lambda=\frac{\max_{\theta \in \Theta_0}L(\theta)}{\max_{\theta \in \Theta}L(\theta)}
\]
The denominator represents the unrestricted maximum of the entire space, including $\Theta_0$.  

Note that by doing this, $\Lambda$ is confined to values between 0 and 1.  

**Example**  Let $X_1, \dots, X_n$ be i.i.d. normal with mean $\mu$ and variance $\sigma^2$ (both unknown) and suppose we want to test $H_0: \mu=\mu_0$ vs. $H_1: \mu \neq \mu_0$.  We have seen the MLEs of $\mu$ and $\sigma^2$.
\[
\hat{\mu}=\overline{X} \ \ \text{and} \ \ \hat{\sigma}^2=\frac{1}{n}\sum_{i=1}^n (X_i - \overline{X})^2
\]
while under $H_0$, $\mu=\mu_0$, and the MLE of $\sigma^2$ is 
\[
\hat{\sigma}^2_0 = \frac{1}{n}\sum_{i=1}^n (X_i - \mu_0)^2
\]
Notice that under $H_0$, 
\[
\begin{aligned}
L(\mu_0, \hat{\sigma}_0^2) &= (\hat{\sigma}_0^2 2\pi)^{-n/2}\exp\bigg(-\sum_{i=1}^n \frac{(X_i - \mu_0)^2}{2\hat{\sigma}_0^2}\bigg) \\
  &= (\hat{\sigma}^2_0 2 \pi)^{-n/2} e^{-n/2}
\end{aligned}
\]
and similarly
\[
L(\hat{\mu}, \hat{\sigma}^2) =(\hat{\sigma}^2 2 \pi)^{-n/2} e^{-n/2}
\]
So the generalized likelihood ratio (LR) is
\[
\begin{aligned}
\Lambda &=\frac{(\hat{\sigma}_0^2 2\pi)^{-n/2}e^{-n/2}}{(\hat{\sigma}^2 2\pi)^{-n/2}e^{-n/2}} \\
 &= \bigg(\frac{\hat{\sigma}_0^2}{\hat{\sigma}^2}\bigg)^{-n/2}
\end{aligned}
\]
And so the likelihood ratio test will reject $H_0$ when $\Lambda \leq k$, where $k$ is calculated so that the test has level $\alpha$.  The distribution of $\Lambda$ is not obvious, but note that $\Lambda$ is a montonic function of $\hat{\sigma}_0^2/\hat{\sigma}^2$ and 
\[
\begin{aligned}
\frac{\hat{\sigma}_0^2}{\hat{\sigma}^2} &= \frac{\frac{1}{n}\sum_{i=1}^n (X_i - \mu_0)^2}{\frac{1}{n}\sum_{i=1}^n (X_i - \overline{X})^2} \\
  &= 1 + \frac{n(\overline{X} - \mu_0)^2}{\sum_{i=1}^n (X_i - \overline{X})^2} \\
  &=1 + \frac{1}{n-1}\bigg(\frac{n(\overline{X} - \mu_0)^2}{s^2}\bigg) \\
  &= 1 + \frac{T^2}{n-1} 
\end{aligned}
\]
where $T=\frac{\overline{X}-\mu}{s/\sqrt{n}}$ and $s$ is the usual sample standard deviation.  We know that, under $H_0$, $T$ has *t*-distribution with $df=n-1$ so that $T^2 \sim F_{(1,n-1)}$.

Since $\Lambda$ is small when $T^2$ is large, we reject for large values of $T^2$ (or $|T|$).

###Large Sample Theory  
**Theorem**: Suppose that $X_1, \dots, X_n$ are i.i.d. random variables with density or mass function satisfying the usual conditions under the MLE framework (these are technical).  If the MLE $\hat{\theta}_n$ satisfies $\sqrt{n}(\hat{\theta}_n - \theta) \xrightarrow{d} N\Big(0, \frac{1}{I(\theta)}\Big)$, then the likelihood ratio statistic $\Lambda$ for testing $H_0: \theta = \theta_0$ satisfies
\[
-2\ln\Lambda = -2\Big[\max_{\theta \in \Theta_0} \ell(\theta) - \max_{\theta \in \Theta} \ell(\theta)\Big] \xrightarrow{d} v \sim \chi^2_{(1)}
\]
when $H_0$ is true.

**Proof** Let $\ell(\theta) = \ln(f(x;\theta))$ and let $\ell'(\theta), \ell''(\theta)$ be its derivatives with respect to $\theta$.  Under the conditions of the theorem, 
\[
\begin{aligned}
&\sqrt{n}(\hat{\theta}_n - \theta) \xrightarrow{d} N\bigg(0, \frac{1}{I(\theta)}\bigg) \\
\Rightarrow &\Big[\sqrt{I(\theta)}\sqrt{n}(\hat{\theta}_n -\theta)\Big]^2 \xrightarrow{d} v \sim \chi^2_{(1)} \\
\Rightarrow &n(\hat{\theta}_n - \theta)^2 \xrightarrow{d} \frac{v}{I(\theta)}
\end{aligned}
\]
Taking the natural log and doing a Taylor series expansion,
\[
\begin{aligned}
-\ln\Lambda &= \sum_{i=1}^n [\ell(\hat{\theta}_n) - \ell(\theta_0)] \\
  &= (\theta_0 - \hat{\theta}_n) \sum_{i=1}^n \ell' (\hat{\theta}_n) - \frac{1}{2}(\theta_0 - \hat{\theta}_n)^2 \sum_{i=1}^n \ell''(\theta^*_n) \\
  &= -\frac{1}{2}n(\hat{\theta}_n - \theta_0)^2 \frac{1}{n} \sum_{i=1}^n \ell'' (\theta^*_n)
\end{aligned}
\]
where $\theta^*_n$ lies between $\hat{\theta}_n$ and $\theta_0$.  Furthermore, when $H_0$ is true,  
\[
-\frac{1}{n} \sum_{i=1}^n \ell''(\theta^*_n) \xrightarrow{p} - E_{\theta_{0}}[\ell''(\theta_0)]=I(\theta_0)
\]
Since $n(\hat{\theta}_n - \theta_0)^2 \xrightarrow{d} \frac{v}{I(\theta_0)}$, the conclusion follows by Slutsky's theorem.

Note: This does have a multidimensional parameter space generalization that we will not discuss.

**Example**  (Geometric Distribution) Consider testing $H_0:p=0.2$ with a sample of $n=20$ observations from a geometric distribution, and suppose the sample mean is $\overline{X}=3$.
\[
\begin{aligned}
L(p) &= (1-p)^{\sum x_i -n}p^n \\
  &= (1-p)^{\sum x_i -20} p^{20} \\
  &= \bigg(\frac{p}{1-p}\bigg)^{20} (1-p)^{20\overline{X}}
\end{aligned}
\]
The value under $H_0$ is $\ln(L(0.2))=-41.11$  
Its unrestricted maximum value attained at the MLE, is $\ln(L(\frac{1}{3}))=-38.19$  
Minus twice the difference yields
\[
-2(-41.11 -(-38.19)) = 5.84
\]
which under $H_0$, approximately follows $\chi^2_{(1)}$.  Then our p-value is
\[
P(\chi^2_{(1)} > 5.84) = 0.016
\]
